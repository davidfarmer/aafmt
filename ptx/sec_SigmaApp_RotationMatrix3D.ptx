<section xml:id="sec_SigmaApp_RotationMatrix3D">
  <title>Rotation matrices (in 3 dimensions)</title>
  <p>
    In three-dimensional space, the <term>dot product</term>
    (or <term>scalar product</term>)
    of two vectors <m>v:=[v_1,v_2,v_3]^{\text{T} }</m> and <m>w:=[w_1,w_2,w_3]^{\text{T} }</m> is defined as
    <me>
      v \cdot w :=  v_1w_1 + v_2w_2 + v_3w_3 = \sum_j v_j w_j
    </me>,
    where we have made use of summation notation to shorten the expression.
    If we also define the <em>length</em><idx><h>Length</h><h>of a vector</h></idx> of the vector <m>v</m>
    (denoted by <m>||v||</m>)
    as
    <me>
      \lVert v\rVert := (v \cdot v)^{1/2}
    </me>,
    then we may then write the <term>cosine formula</term> as
    <me>
      \cos(\theta) = \frac{v \cdot w}{\lVert v \rVert \lVert w \rVert}
    </me>,
    where <m>\theta</m> is the angle between the two vectors <m>v</m> and <m>w</m>.
    (You may have encountered this formula in physics class or precalculus.)
  </p>
  <p>
    Any <m>3 \times 3</m> matrix <m>A</m> produces a function from three-dimensional space to itself as follows:
    given any vector <m>v:=[v_1,v_2,v_3]^{\text{T} }</m>,
    then the image vector is <m>Av</m>.
    Using summation notation, we may write:
    <me>
      [Av]_i = \sum_j A_{ij} v_j
    </me>.
  </p>
  <p>
    A matrix is called a <term>rotation matrix</term>
    if it satisfies the following properties:
    <ul>
      <li>
        <p>
          A rotation matrix <m>R</m> must
          <em>preserve  lengths and angles</em>.
          In other words,
          if <m>v</m> and <m>w</m> are any two 3-d vectors , then  <m>\lVert Rv \rVert = \lVert v \rVert</m>,
          <m>\lVert Rw \rVert = \lVert v \rVert</m>,
          and furthermore the angle between <m>Rv</m> and <m>Rw</m> must be the same as the angle between <m>v</m> and <m>w</m>.
          In view of the cosine formula,
          this means that the dot product must be preserved:
          <m>Rv \cdot Rw = v \cdot w</m>.
          In fact, since vector length is the square root of a dot product,
          all of these conditions will be satisfied as long as
          <me>
            Rv \cdot Rw = v \cdot w
          </me>
          for any two 3-d vectors <m>v</m> and <m>w</m>.
        </p>
      </li>
      <li>
        <p>
          A rotation matrix must <em>preserve handedness</em>.
          This means that if the vectors
          <m>\{ [1,0,0]^{\text{T} }, [0,1,0]^{\text{T} }, [0,0,1]^{\text{T} } \}</m> form a right-handed coordinate system,
          then the image vectors <m>\{ R[1,0,0]^{\text{T} }, R[0,1,0]^{\text{T} }, R[0,0,1]^{\text{T} } \}</m> must also form a right-handed coordinate system.
        </p>
      </li>
    </ul>
  </p>
  <p>
    It turns out that this second condition implies that <m>\det(R)\ge 0</m>.
    We won't prove this,
    but we can give a few examples to show that it is reasonable.
    Consider the <m>3 \times 3</m> matrix <m>-I</m>,
    which has determinant equal to <m>-1</m>.
    This matrix will map the <m>x</m>, <m>y</m>,
    and <m>z</m> axes to the <m>-x</m>, <m>-y</m>,
    and <m>-z</m> axes respectively.
    By using the right-hand rule,
    you may verify that if the <m>x</m>, <m>y</m>,
    and <m>z</m> axes form a right-handed coordinate system,
    then the <m>-x</m>,
    <m>-y</m>, and <m>-z</m> axes form a left-handed coordinate system.
  </p>
  <p>
    The following exercise gives some other examples of matrices <m>R</m> with
    <m>\det(R)\lt 0</m> which do not preserve handedness.
  </p>
  <exercise>
    <statement>
      <ol type="a">
        <li>
          <p>
            Find a matrix which maps the <m>x</m>, <m>y</m>,
            and <m>z</m> axes to the <m>-x</m>, <m>y</m>,
            and <m>z</m> axes respectively.
            What's its determinant?
          </p>
        </li>
        <li>
          <p>
            show that the function defined in (a) maps a right-handed coordinate system to a left-handed coordinate system.
          </p>
        </li>
        <li>
          <p>
            Repeat parts (a) and (b) for the case where the <m>x</m>,
            <m>y</m>,
            and <m>z</m> axes to the <m>x</m>, <m>-y</m>,
            and <m>z</m> axes respectively.
          </p>
        </li>
      </ol>
    </statement>
  </exercise>
  <p>
    So let's go back to the first condition for rotation matrices,
    namely that they preserve inner products:
    <m>Rv \cdot Rw = v \cdot w</m>.
    Let's rewrite this in coordinate notation.
    First, note that <m>[Rv]_k</m> and <m>[Rw]_k</m> can be written as
    <m>\sum_i r_{ki}v_i</m> and <m>\sum_j r_{kj} w_j</m> respectively,
    where <m>r_{kj}</m> is the <m>(k,j)</m> entry of <m>R</m>.
    Therefore we have:
    <md>
      <mrow>Rv \cdot Rw \amp = \sum_k  [Rv]_k[Rw]_k</mrow>
      <mrow>\amp = \sum_k \left(\sum_i r_{ki}v_i \right)\left(\sum_j r_{kj}w_j \right)]</mrow>
      <mrow>\amp = \sum_{i,j,k} (r_{ki}v_i)(r_{kj}w_j)</mrow>
      <mrow>\amp = \sum_{i,j,k} r_{ki}r_{kj} v_i w_j</mrow>
    </md>.
  </p>
  <p>
    Recall our rotation condition:
    <m>Rv \cdot Rw = v\cdot w</m>,
    which must be true for any two vectors <m>v</m> and <m>w</m>.
    In summation notation, this becomes:
    <me>
      \sum_{i,j,k} r_{ki}r_{kj} v_i w_j =  \sum_m  v_m w_m
    </me>
  </p>
  <p>
    Now let's consider different possibilities for <m>v</m> and <m>w</m>.
    For example we may let <m>v=w=[1,0,0]^{\text{T} }</m>. this means that
    <m>v_i = \delta_{i1}</m> and <m>w_j = \delta_{j1}</m>,
    where <m>\delta</m> is our old friend the Kronecker delta.
    Plugging this into our summation notation expression gives:
    <me>
      \sum_{i,j,k} r_{ki}r_{kj} \delta_{i1}\delta_{j1} =  \sum_m  \delta_{m1}\delta_{m1}
    </me>.
  </p>
  <p>
    Because of the <m>\delta</m>'s, when we sum over <m>i,j</m>,
    and <m>m</m> the only terms that contribute will be <m>i=j=m=1</m>.
    In summary, we obtain:
    <me>
      \sum_{k} r_{k1}r_{k1}  = 1
    </me>.
  </p>
  <p>
    Using this strategy, we can obtain a whole bunch of identities:
  </p>
  <exercise>
    <statement>
      <ol type="a">
        <li>
          <p>
            By plugging in different possibilities for <m>v</m> and <m>w</m>,
            show that if
            <me>
              \sum_{i,j,k} r_{ki}r_{kj} v_i w_j =  \sum_m  v_m w_m
            </me>
            holds for all <m>v, w</m> then
            <me>
              \sum_{k} r_{ki}r_{kj}  = \delta_{ij}
            </me>.
          </p>
        </li>
        <li>
          <p>
            Show the converse of (a), namely:  given that
            <me>
              \sum_{k} r_{ki}r_{kj}  = \delta_{ij}
            </me>,
            show that
            <me>
              \sum_{i,j,k} r_{ki}r_{kj} v_i w_j =  \sum_m  v_m w_m
            </me>
            for all <m>v, w</m>.
          </p>
        </li>
        <li>
          <p>
            Show that the expression <m>\sum_{k} r_{ki}r_{kj}  = \delta_{ij}</m> can be rewritten in matrix form as:
            <me>
              R^{\text{T} }R = I
            </me>.
          </p>
        </li>
      </ol>
    </statement>
  </exercise>
  <p>
    We summarize these results in a proposition:
  </p>
  <proposition xml:id="proposition_SigmaNotation_rotTrans">
    <statement>
      <p>
        A <m>3 \times 3</m> matrix <m>R</m> is a rotation matrix if and only if
        <m>\det(R)>0</m> and <m>R^{\text{T} }R = I</m>.
      </p>
    </statement>
  </proposition>
  <p>
    We will pick up on rotation matrices in <xref ref="subsec_SigmaApp_LeviCivitaAndApp_LeviCivitaDet">Section</xref>
    when we talk about determinants,
    and again in <xref ref="subsec_SigmaApp_LeviCivitaAndApp_EulerRot">Section</xref>
    when we prove Euler's rotation theorem.
  </p>
</section>